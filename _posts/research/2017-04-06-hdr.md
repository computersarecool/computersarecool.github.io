---
layout: project_image_and_vimeo
permalink: /:title/
category: research

meta:
  keywords: "HDR, Photography, Camera Response Function, Image Processing, Research, Computational Photography"

project:
  title: "HDR - Camera Response Function Recovery"
  url: "https://willynolan.com/hdr"
  logo_type: "video"
  logo: "/assets/media/research/hdr/logo.webm"
  logo_backup: "/assets/media/research/hdr/logo.mp4"

images:
  - image:
    url: "/assets/media/research/hdr/first.png"
    alt: "HDR image created from a radiance map"
  - image:
    url: "/assets/media/research/hdr/second.png"
    alt: "Image acquisition pipeline and false color image of radiance map"
  - image:
    url: "/assets/media/research/hdr/third.png"
    alt: "Images at different shutter speeds"
videos:
---
<p>
Today High Dynamic Range Images, or HDRI are commonplace. Many smartphones contain built-in support for HDR images and 
some even default to using HDR to capture images.
</p>

<p>
Fundamentally dealing with HDR data is all about capturing the radiance of the scene. A naive thought would be that 
pixels appearing twice as bright in an image relate to areas in the photographed scene with twice as much radiance.
</p>

<p>
The second featured image shows this is not the case. A tremendous range of radiance values have to be mapped to 
a significantly smaller range of pixel intensities. The image also shows the pipeline that creates this non-linear 
mapping.
</p>

<p>
For many reasons this is not true, and one challenging reason is the non-linear response function of cameras. This 
function is not usually published by camera makers who consider it a trade secret. 
</p>

<p>
Recovering this function allows more than just the the common HDRI-style images (and their hyper-realistic 
tone-mapped counterparts), including being able to achieve more realistic motion-blur, color-correction and edge 
detection in images. 
</p>

<p>
With a stack of aligned images at different exposures, the non-linear response function can be recovered in the
following process.
</p>

<p>
$$
Z_{ij} = f(E\Delta t)
$$
Where $i$ indexes over pixel locations and $j$ indexes over the different equations.
</p>

<p>
With some manipulation this simplifies to:
$$
g(Z_{ij}) = \ln E_i + \ln \Delta t_j
$$
Where $g = \ln f^{-1}$
</p>

<p>
Written in this form $g$ (and therefore eventually $f$) can be recovered up to a scale by using the $SVD$ to minimize 
a related objective function. 
</p>

<p>
With this function recovered the radiance can be visualized a number of ways (for instance usually by scaling the radiance
to the display device).
</p>

<p>
This project implements the following academic papers:
<ul>
    <li>
        <a href="https://dl.acm.org/doi/10.1145/1401132.1401174">Recovering High Dynamic Range Radiance Maps from Photographs</a> by Paul E. Debevec and Jitendra M. Malik
    </li>
    <li>
        <a href="https://ieeexplore.ieee.org/document/1240119">Determining the camera response from images: what is knowable?</a> M.D. Grossberg, S.K. Nayar
     </li>
     <li>
         <a href="https://ieeexplore.ieee.org/document/1323796">Modeling the space of camera response functions</a> M.D. Grossberg, S.K. Nayar
      </li>
</ul>
